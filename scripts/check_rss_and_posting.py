"""æŒ‡å®šã—ãŸæ™‚é–“å¹…å†…ã§RSSãƒ•ã‚£ãƒ¼ãƒ‰ã®æ›´æ–°ã‚’ãƒã‚§ãƒƒã‚¯ã—ã¦Tweetã™ã‚‹ã‚¹ã‚¯ãƒªãƒ—ãƒˆ"""

import json
import logging
import os
import sys
import time
from datetime import datetime, timedelta, timezone
import re

import feedparser
import tweepy
# from google import genai
# from google.genai import types


logging.basicConfig(level=logging.DEBUG, format="%(asctime)s [%(levelname)s] %(name)s: %(message)s")

TWEET_URL_LENGTH = 23
MAX_TWEET_LENGTH = 159  # Xï¼ˆæ—§Twitterï¼‰ã®ãƒ„ã‚¤ãƒ¼ãƒˆã®æœ€å¤§æ–‡å­—æ•°
RSS_VIEWER_URL = "https://testkun08080.github.io/kanpo-rss"
DEBUG = os.getenv("DEBUG_CHECK", False)

# def ping_to_gemini(prompt: str) -> str:
#     """Gemini API ã«ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’é€ä¿¡ã—ã€å¿œç­”ã‚’è¿”ã™é–¢æ•°ã€‚

#     Google ã® Gemini ãƒ¢ãƒ‡ãƒ«ã€Œgemini-2.5-flashã€ã‚’ä½¿ç”¨ã—ã¦ã€
#     æŒ‡å®šã—ãŸãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã«åŸºã¥ããƒ†ã‚­ã‚¹ãƒˆã‚’ç”Ÿæˆã—ã¾ã™ã€‚
#     APIã‚­ãƒ¼ã¯ç’°å¢ƒå¤‰æ•° `GEMINI_API_KEY` ã‹ã‚‰å–å¾—ã—ã¾ã™ã€‚
#     å‡¦ç†é€Ÿåº¦ã‚’å„ªå…ˆã™ã‚‹ãŸã‚ "thinking" æ©Ÿèƒ½ã¯ç„¡åŠ¹åŒ–ã—ã¦ã„ã¾ã™ã€‚

#     Args:
#         prompt (str): Gemini ã«é€ä¿¡ã™ã‚‹ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆï¼ˆè³ªå•ã‚„æŒ‡ç¤ºæ–‡ï¼‰ã€‚

#     Returns:
#         str: Gemini ã‹ã‚‰è¿”ã£ã¦ããŸå¿œç­”ãƒ†ã‚­ã‚¹ãƒˆã€‚

#     Raises:
#         EnvironmentError: ç’°å¢ƒå¤‰æ•° `GEMINI_API_KEY` ãŒè¨­å®šã•ã‚Œã¦ã„ãªã„å ´åˆã€‚
#         Exception: Gemini API ãƒªã‚¯ã‚¨ã‚¹ãƒˆãŒå¤±æ•—ã—ãŸå ´åˆãªã©ã€‚
#     """
#     api_key = os.environ.get("GEMINI_API_KEY")
#     if not api_key:
#         raise EnvironmentError("ç’°å¢ƒå¤‰æ•° GEMINI_API_KEY ãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚")

#     try:
#         client = genai.Client(api_key=api_key)
#         response = client.models.generate_content(
#             model="gemini-2.5-flash-lite",
#             contents=prompt,
#             config=types.GenerateContentConfig(
#                 thinking_config=types.ThinkingConfig(thinking_budget=0)  # thinkingç„¡åŠ¹åŒ–
#             ),
#         )
#         return response.text
#     except Exception as e:
#         return f"ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {str(e)}"


def clean_duplicate_tags(text):
    """ãƒã‚¹ãƒˆç”¨ã®ã‚¿ã‚°é‡è¤‡ã‚’å‰Šé™¤ã—ã€æœ¬æ–‡ã®æ”¹è¡Œã¯ä¿æŒã—ã¦æ•´å½¢ã™ã‚‹é–¢æ•°."""
    # ãƒãƒƒã‚·ãƒ¥ã‚¿ã‚°æŠ½å‡ºï¼ˆæœ¬æ–‡ã‹ã‚‰é™¤å»ã™ã‚‹ãŸã‚ã®ãƒªã‚¹ãƒˆï¼‰
    tags = re.findall(r"#\S+", text)

    # é‡è¤‡å‰Šé™¤ï¼ˆé †åºä¿æŒï¼‰
    seen = set()
    unique_tags = []
    for tag in tags:
        if tag not in seen:
            seen.add(tag)
            unique_tags.append(tag)

    # æœ¬æ–‡éƒ¨åˆ†ï¼ˆã‚¿ã‚°ã¯å‰Šé™¤ã™ã‚‹ãŒæ”¹è¡Œã¯ä¿æŒï¼‰
    lines = text.splitlines()
    non_tag_lines = []
    for line in lines:
        cleaned_line = re.sub(r"#\S+", "", line)
        non_tag_lines.append(cleaned_line)

    # æœ¬æ–‡ãã®ã¾ã¾çµåˆï¼ˆä½™è¨ˆãªæ”¹è¡Œæ•´å½¢ã¯ã—ãªã„ï¼‰
    non_tag_text = "\n".join(non_tag_lines).rstrip()

    # æœ¬æ–‡ + ã‚¿ã‚°ï¼ˆæœ¬æ–‡æœ€å¾Œã«å¿…ãš1è¡Œç©ºã‘ã¦ã‚¿ã‚°ï¼‰
    return f"{non_tag_text}\n\n" + "\n".join(unique_tags)


def count_tweet_length(text: str) -> int:
    """
    Twitterã®æ–‡å­—æ•°ã‚«ã‚¦ãƒ³ãƒˆä»•æ§˜ã«æº–æ‹ ã—ã¦ã€ãƒ†ã‚­ã‚¹ãƒˆã®é•·ã•ã‚’è¿”ã™ã€‚
    URLã¯ã™ã¹ã¦23æ–‡å­—ã¨ã—ã¦ã‚«ã‚¦ãƒ³ãƒˆã™ã‚‹ã€‚

    Args:
        text (str): ãƒ„ã‚¤ãƒ¼ãƒˆãƒ†ã‚­ã‚¹ãƒˆ

    Returns:
        Optional[int]: æŠ•ç¨¿ã•ã‚ŒãŸãƒ„ã‚¤ãƒ¼ãƒˆã®IDã€‚å¤±æ•—ã—ãŸå ´åˆã¯Noneã€‚
    """
    # URLã‚’è¦‹ã¤ã‘ã¦ãã®éƒ¨åˆ†ã‚’23æ–‡å­—ã«æ›ç®—
    url_regex = re.compile(r"https?://\S+")
    adjusted_text = text
    for url_match in url_regex.finditer(text):
        url = url_match.group(0)
        adjusted_text = adjusted_text.replace(url, "X" * TWEET_URL_LENGTH, 1)
    return len(adjusted_text)


def post_to_x(text, in_reply_to_tweet_id=None):
    """
    ãƒ†ã‚­ã‚¹ãƒˆã‚’Xï¼ˆæ—§Twitterï¼‰ã«æŠ•ç¨¿ã—ã¾ã™ã€‚tweepyï¼ˆTwitter API v2ï¼‰ã‚’ä½¿ç”¨ã—ã¾ã™ã€‚
    in_reply_to_tweet_idãŒæŒ‡å®šã•ã‚ŒãŸå ´åˆã¯ã€ãƒªãƒ—ãƒ©ã‚¤ã¨ã—ã¦æŠ•ç¨¿ã—ã¾ã™ã€‚
    æŠ•ç¨¿ã«æˆåŠŸã—ãŸå ´åˆã¯ãƒ„ã‚¤ãƒ¼ãƒˆIDã‚’è¿”ã—ã€å¤±æ•—ã—ãŸå ´åˆã¯Noneã‚’è¿”ã—ã¾ã™ã€‚

    Args:
        text (str): æŠ•ç¨¿ã™ã‚‹ãƒ„ã‚¤ãƒ¼ãƒˆã®æœ¬æ–‡ã€‚
        in_reply_to_tweet_id (Optional[int]): ãƒªãƒ—ãƒ©ã‚¤å…ˆã®ãƒ„ã‚¤ãƒ¼ãƒˆIDï¼ˆçœç•¥å¯ï¼‰ã€‚

    Returns:
        Optional[int]: æŠ•ç¨¿ã•ã‚ŒãŸãƒ„ã‚¤ãƒ¼ãƒˆã®IDã€‚å¤±æ•—ã—ãŸå ´åˆã¯Noneã€‚
    """

    logging.info(f":-------------------Tweetå†…å®¹:-------------------")

    if in_reply_to_tweet_id:
        text = clean_duplicate_tags(text)
    logging.info(text)

    if DEBUG:
        return 1

    # bearer_token = os.environ.get("BEARER_TOKEN")
    api_key = os.environ.get("X_API_KEY")
    api_secret = os.environ.get("X_API_SECRET")
    access_token = os.environ.get("X_ACCESS_TOKEN")
    access_token_secret = os.environ.get("X_ACCESS_TOKEN_SECRET")
    if not (api_key and api_secret and access_token and access_token_secret):
        logging.error("Twitter APIã®èªè¨¼æƒ…å ±ãŒç’°å¢ƒå¤‰æ•°ã«è¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚")
        return None

    client = tweepy.Client(
        consumer_key=api_key,
        consumer_secret=api_secret,
        access_token=access_token,
        access_token_secret=access_token_secret,
        wait_on_rate_limit=True,
    )

    tweet_id = None  # å…ˆã«å®šç¾©ã—ã¦ãŠãã“ã¨ã§ finally ã§ã‚‚å‚ç…§å¯èƒ½

    try:
        if in_reply_to_tweet_id:
            response = client.create_tweet(
                text=text,
                in_reply_to_tweet_id=in_reply_to_tweet_id,
            )
        else:
            response = client.create_tweet(text=text)

        tweet_id = response.data["id"]

    except Exception as e:
        logging.error(f"Tweetã«å¤±æ•— tweet: {e}")
        tweet_id = None

    finally:
        logging.info(f"Tweet ID: {tweet_id}")
        logging.info(f":-------------------Tweetå†…å®¹End:-------------------")

    return tweet_id


def main():
    """
    RSSãƒ•ã‚£ãƒ¼ãƒ‰ã‚’ãƒã‚§ãƒƒã‚¯ã—ã€æŒ‡å®šã—ãŸæ™‚é–“å¹…å†…ã«æ›´æ–°ã•ã‚ŒãŸã‚¨ãƒ³ãƒˆãƒªã‚’æŠ½å‡ºã—ã¦Xï¼ˆæ—§Twitterï¼‰ã«æŠ•ç¨¿ã—ã¾ã™ã€‚
    ã¾ãŸã€é–¢é€£ã™ã‚‹è¦ç´„æƒ…å ±ãŒã‚ã‚Œã°ãƒªãƒ—ãƒ©ã‚¤ã¨ã—ã¦æŠ•ç¨¿ã—ã¾ã™ã€‚GitHub Actionsç”¨ã®å‡ºåŠ›ã‚‚è¡Œã„ã¾ã™ã€‚

    ã‚³ãƒãƒ³ãƒ‰ãƒ©ã‚¤ãƒ³å¼•æ•°:
        rss_url (str): ãƒã‚§ãƒƒã‚¯å¯¾è±¡ã®RSSãƒ•ã‚£ãƒ¼ãƒ‰URLã€‚
        rss_toc_url (str): é–¢é€£æƒ…å ±å–å¾—ç”¨ã®RSSãƒ•ã‚£ãƒ¼ãƒ‰URLã€‚
        minutes (int): ä½•åˆ†å‰ã‹ã‚‰ã®æ›´æ–°ã‚’ãƒã‚§ãƒƒã‚¯ã™ã‚‹ã‹ã€‚

    ç’°å¢ƒå¤‰æ•°:
        BEARER_TOKEN: X Bearerãƒˆãƒ¼ã‚¯ãƒ³ã€‚
        X_API_KEY: X APIã‚­ãƒ¼ã€‚
        X_API_SECRET: X APIã‚­ãƒ¼ã‚·ãƒ¼ã‚¯ãƒ¬ãƒƒãƒˆã€‚
        X_ACCESS_TOKEN: Xã‚¢ã‚¯ã‚»ã‚¹ãƒˆãƒ¼ã‚¯ãƒ³ã€‚
        X_ACCESS_TOKEN_SECRET: Xã‚¢ã‚¯ã‚»ã‚¹ãƒˆãƒ¼ã‚¯ãƒ³ã‚·ãƒ¼ã‚¯ãƒ¬ãƒƒãƒˆã€‚
        GITHUB_OUTPUT: GitHub Actionsç”¨ã®å‡ºåŠ›ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹ï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰ã€‚

    å‡¦ç†å†…å®¹:
        1. RSSãƒ•ã‚£ãƒ¼ãƒ‰ã‹ã‚‰æŒ‡å®šæ™‚é–“å¹…å†…ã®æ–°è¦ã‚¨ãƒ³ãƒˆãƒªã‚’æŠ½å‡ºã€‚
        2. æ–°è¦ã‚¨ãƒ³ãƒˆãƒªãŒã‚ã‚Œã°Xã«æŠ•ç¨¿ã—ã€é–¢é€£ã™ã‚‹è¦ç´„æƒ…å ±ãŒã‚ã‚Œã°ãƒªãƒ—ãƒ©ã‚¤ã¨ã—ã¦æŠ•ç¨¿ã€‚
        3. GitHub Actionsç”¨ã«æ›´æ–°æœ‰ç„¡ã¨ã‚¨ãƒ³ãƒˆãƒªæƒ…å ±ã‚’å‡ºåŠ›ã€‚

    ä¾‹å¤–:
        Twitter APIã®èªè¨¼æƒ…å ±ãŒä¸è¶³ã—ã¦ã„ã‚‹å ´åˆã¯æŠ•ç¨¿ã‚’ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã™ã€‚
        æ–°è¦ã‚¨ãƒ³ãƒˆãƒªãŒãªã„å ´åˆã¯è­¦å‘Šã‚’å‡ºåŠ›ã—ã¾ã™ã€‚
    """
    logging.basicConfig(level=logging.INFO)

    rss_url = sys.argv[1]
    rss_toc_url = sys.argv[2]
    minutes = int(sys.argv[3])
    diff_time = datetime.now(timezone.utc) - timedelta(minutes=minutes)

    logging.info(f"RSS URL: {rss_url}")
    logging.info(f"RSS_toc URL: {rss_toc_url}")
    logging.info(f"ãƒã‚§ãƒƒã‚¯æ™‚é–“å¹…: {minutes}åˆ†å‰ = {diff_time.isoformat()}ä»¥é™")

    feed = feedparser.parse(rss_url)
    feed_toc = feedparser.parse(rss_toc_url)
    updated_entries = []
    updated_toc_entries = []

    # RSSãƒ•ã‚£ãƒ¼ãƒ‰ã®ã‚¨ãƒ³ãƒˆãƒªã‚’ãƒã‚§ãƒƒã‚¯ã—ã¦ã€æ›´æ–°å¿…è¦åˆ†ã ã‘ã«ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ã™ã‚‹
    for entry in feed.entries:
        if hasattr(entry, "published_parsed"):
            pub_dt = datetime(*entry.published_parsed[:6], tzinfo=timezone.utc)
        else:
            continue  # Skip items without pubDate

        if pub_dt >= diff_time:
            updated_entries.append({
                "title": entry.get("title", ""),
                "link": entry.get("link", ""),
                "summary": entry.get("summary", ""),
                "pubDate": pub_dt.strftime("%Y-%m-%d %H:%M:%S, GMT"),
            })

    updated = bool(updated_entries)

    # RSS_TOC(è©³ç´°ç‰ˆ)ã‚‚ãƒ•ã‚£ãƒ¼ãƒ‰ã®ã‚¨ãƒ³ãƒˆãƒªã‚’ãƒã‚§ãƒƒã‚¯ã—ã¦ã€æ›´æ–°å¿…è¦åˆ†ã ã‘ã«ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ã™ã‚‹
    for entry in feed_toc.entries:
        if hasattr(entry, "published_parsed"):
            pub_dt = datetime(*entry.published_parsed[:6], tzinfo=timezone.utc)
        else:
            continue  # Skip items without pubDate

        if pub_dt >= diff_time:
            updated_toc_entries.append({
                "title": entry.get("title", ""),
                "link": entry.get("link", ""),
                "summary": entry.get("summary", ""),
                "pubDate": pub_dt.strftime("%Y-%m-%d %H:%M:%S, GMT"),
                "categories": [tag["term"] for tag in entry.get("tags", [])],
            })
    logging.info(f"æ›´æ–°ã•ã‚ŒãŸRSSãƒ•ã‚£ãƒ¼ãƒ‰ã®ã‚¨ãƒ³ãƒˆãƒªæ•°: {len(updated_entries)}")
    logging.info(f"æ›´æ–°ã•ã‚ŒãŸRSS_TOCã®ã‚¨ãƒ³ãƒˆãƒªæ•°: {len(updated_toc_entries)}")

    # --- X (Twitter) posting ---
    base_tags = ["#å®˜å ±", "#å®˜å ±é€šçŸ¥"]

    if not DEBUG:
        required_env = ["X_API_KEY", "X_API_SECRET", "X_ACCESS_TOKEN", "X_ACCESS_TOKEN_SECRET"]
    else:
        required_env = []
    if all(os.environ.get(env_key) for env_key in required_env):
        if updated:
            for entry in updated_entries:
                # ãƒ„ã‚¤ãƒ¼ãƒˆå†…å®¹ã‚’ä½œæˆ
                # extra = "å„é …ç›®ã®ãƒªãƒ³ã‚¯ãªã©ã¯ä»¥ä¸‹ãƒªãƒ—ãƒ©ã‚¤ã‚’ã”è¦§ãã ã•ã„.."
                extra = f"ä»¥ä¸‹RSSãƒ“ãƒ¥ãƒ¼ãƒ¯ãƒ¼webã§é …ç›®ã”ã¨ã§è¦‹ã‚‹ã“ã¨ã‚‚å¯èƒ½ã§ã™ã€‚ã€‚ã€‚\n{RSS_VIEWER_URL}"
                tweet_text = f"ğŸ“š{entry['title']}\n{entry['link']}\n\n{' '.join(base_tags)}\n\n{extra}"
                tweet_id = post_to_x(tweet_text)
                if tweet_id is None:
                    continue

                # feed_tocã‹ã‚‰ã‚¿ã‚°ã‚’æŠ½å‡º
                batch_text = f"{entry['title']}\n{entry['link']}\n\n"
                serch_entries = [e for e in updated_toc_entries if entry["title"] in e.get("summary", "")]
                for toc_entry in serch_entries:
                    summary = toc_entry.get("summary", "")
                    categories = toc_entry.get("categories", [])

                    if entry["title"] not in summary:
                        logging.info(f"{entry['title']} not in {summary}")
                        continue

                    entry_text = ""
                    if categories:
                        categories_tags = " ".join([f"#{cat}" for cat in categories])
                        entry_text = f"{categories_tags}\n"
                    else:
                        continue

                    # ã“ã®entryã‚’è¿½åŠ ã—ãŸã‚‰æ–‡å­—æ•°åˆ¶é™ã‚’è¶…ãˆã‚‹ã‹ï¼Ÿ
                    if count_tweet_length(batch_text + entry_text) > MAX_TWEET_LENGTH:
                        post_to_x(batch_text.strip(), in_reply_to_tweet_id=tweet_id)
                        time.sleep(1)
                        batch_text = entry_text
                    else:
                        batch_text += entry_text

                # æœ€å¾Œã«æ®‹ã£ã¦ã„ãŸã‚‰æŠ•ç¨¿
                if batch_text:
                    post_to_x(batch_text.strip(), in_reply_to_tweet_id=tweet_id)

                # # feed_tocã‹ã‚‰é–¢é€£æƒ…å ±ã‚’æ¢ã—ã¦ãƒªãƒ—ãƒ©ã‚¤
                # batch_text = ""
                # serch_entries = [e for e in updated_toc_entries if entry["title"] in e.get("summary", "")]
                # for toc_entry in serch_entries:
                #     summary = toc_entry.get("summary", "")
                #     categories = toc_entry.get("categories", [])

                #     if entry["title"] not in summary:
                #         logging.info(f"{entry['title']} not in {summary}")
                #         continue

                #     reply_title = toc_entry.get("title", "")
                #     reply_link = toc_entry.get("link", "")

                #     if categories:
                #         categories_tags = " ".join([f"#{cat}" for cat in categories])
                #         entry_text = f"âœ{reply_title}\n{reply_link}\n{categories_tags}\n\n"
                #     else:
                #         entry_text = f"âœ{reply_title}\n{reply_link}\n\n"

                #     # ã“ã®entryã‚’è¿½åŠ ã—ãŸã‚‰æ–‡å­—æ•°åˆ¶é™ã‚’è¶…ãˆã‚‹ã‹ï¼Ÿ
                #     if count_tweet_length(batch_text + entry_text) > MAX_TWEET_LENGTH:
                #         post_to_x(batch_text.strip(), in_reply_to_tweet_id=tweet_id)
                #         time.sleep(1)
                #         batch_text = entry_text
                #     else:
                #         batch_text += entry_text

                # # æœ€å¾Œã«æ®‹ã£ã¦ã„ãŸã‚‰æŠ•ç¨¿
                # if batch_text:
                #     post_to_x(batch_text.strip(), in_reply_to_tweet_id=tweet_id)

                # ãƒªãƒ—ãƒ©ã‚¤ã®é–“éš”ã‚’ç©ºã‘ã‚‹
                time.sleep(2)
        else:
            logging.warning("RSSãƒ•ã‚£ãƒ¼ãƒ‰ã®ã‚¢ãƒƒãƒ—ãƒ‡ãƒ¼ãƒˆãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“.")
    else:
        logging.warning("Twwitter APIã®èªè¨¼æƒ…å ±ãŒä¸è¶³ã—ã¦ã„ã¾ã™ã€‚æŠ•ç¨¿ã‚’ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã™ã€‚")

    # çµæœã®å‡ºåŠ›
    if "GITHUB_OUTPUT" in os.environ:
        output_path = os.environ["GITHUB_OUTPUT"]
        with open(output_path, "a") as fh:
            print(f"updated={'true' if updated else 'false'}", file=fh)
            print(f"entries={json.dumps(updated_entries, ensure_ascii=False)}", file=fh)
    else:
        result = {"updated": updated, "entries": updated_entries}
        print(json.dumps(result, ensure_ascii=False, indent=2))


if __name__ == "__main__":
    main()
